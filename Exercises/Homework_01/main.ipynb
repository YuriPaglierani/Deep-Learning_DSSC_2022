{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us suppose we wish to build a larger model from the graph below.\n",
    "\n",
    "![](imgs/01/mlp_graph_larger.jpg)\n",
    "\n",
    "We suppose that\n",
    "\n",
    "1. The layers have no bias units\n",
    "2. The activation function for hidden layers is `ReLU`\n",
    "\n",
    "    $ \\text{ReLU}(x) = \\max(0, x)$\n",
    "\n",
    "Moreover, we suppose that this is a classification problem.\n",
    "\n",
    "As you might recall, when the number of classes is > 2, we encode the problem in such a way that the output layer has a no. of neurons corresponding to the no. of classes. Doing so, we establish a correspondence between output units and classes. The value of the $j$-th neuron represents the **confidence** of the network in assigning a given data instance to the $j$-th class.\n",
    "\n",
    "Classically, when the network is encoded in such way, the activation function for the final layer is the **softmax** function.\n",
    "If $C$ is the total number of classes,\n",
    "\n",
    "$softmax(z_j) = \\frac{\\exp(z_j)}{\\sum_{k=1}^C \\exp(z_k)}$\n",
    "\n",
    "where $j\\in \\{1,\\cdots,C\\}$ is one of the classes.\n",
    "\n",
    "If we repeat this calculation for all $j$ s, we end up with $C$ normalized values (i.e., between 0 and 1) which can be interpreted as a confidence that the network has in assigning the instance to the corresponding class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homework\n",
    "\n",
    "1. build the MLP in the image above using PT built-ins\n",
    "2. Provide calculation for the exact number of parameters of the MLP\n",
    "   - Do it first supposing that the layers don't have a bias term, then supposing that the bias is present wherever it's possible\n",
    "3. Calculate the $L_1$ (vectorial) norm and the Frobenius norm for the params of each layer\n",
    "4. Given 10 random datapoints, feed them into the network. This operation must be done all in one single command and must **not** make use of loops.\n",
    "   - Given the output of the network, using PyTorch code, find the class of assignment of each datapoint. This also must be done in a single PyTorch command without using loops.\n",
    "   - Drafting a vector of ground truths (whichever labels you like), provide code for the calculation of the accuracy\n",
    "     - Tip: first get the number of correct assignments, then..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of the number of hyperparameters:\n",
    "1. We recall we have no bias, moreover our NNet is dense -> fully connected\n",
    "2. Evaluation of the number of parameters:\n",
    "\n",
    "-> 5*11 = 55 connections between the 1 st and the 2 nd layer;\n",
    "\n",
    "-> 11*16 = 176 connections between the 2 nd and the 3 rd layer;\n",
    "\n",
    "-> 16*13 = 208 connections between the 3 rd and the 4 th layer;\n",
    "\n",
    "-> 13*8 = 104 connections between the 4 th and the 5 th layer;\n",
    "\n",
    "-> 8*4 = 32 connections between the 5 th and the 6 th layer.\n",
    "##### Totally, if we suppose no bias we have 575 hyperparameters\n",
    "\n",
    "3. Now, let's suppose we have no bias:\n",
    "\n",
    "-> (5+1)*11 = 66 connections between the 1 st and the 2 nd layer;\n",
    "\n",
    "-> (11+1)*16 = 192 connections between the 2 nd and the 3 rd layer;\n",
    "\n",
    "-> (16+1)*13 = 221 connections between the 3 rd and the 4 th layer;\n",
    "\n",
    "-> (13+1)*8 = 112 connections between the 4 th and the 5 th layer;\n",
    "\n",
    "-> (8+1)*4 = 36 connections between the 5 th and the 6 th layer.\n",
    "##### Totally, if we suppose no bias we have 627 hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MLP_hw                                   --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       55\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Linear: 2-3                       176\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       208\n",
       "│    └─ReLU: 2-6                         --\n",
       "│    └─Linear: 2-7                       104\n",
       "│    └─ReLU: 2-8                         --\n",
       "│    └─Linear: 2-9                       32\n",
       "│    └─Softmax: 2-10                     --\n",
       "=================================================================\n",
       "Total params: 575\n",
       "Trainable params: 575\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building our model\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "class MLP_hw(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(5, 11, bias=False),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(11, 16, bias=False),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 13, bias=False),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(13, 8, bias=False),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(8, 4, bias=False),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "my_model = MLP_hw()\n",
    "summary(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer number 1: torch.Size([11, 5])\n",
      "L1 norm: 12.364631652832031\n",
      "\n",
      "\n",
      "Layer number 2: torch.Size([16, 11])\n",
      "L1 norm: 26.224891662597656\n",
      "\n",
      "\n",
      "Layer number 3: torch.Size([13, 16])\n",
      "L1 norm: 25.478038787841797\n",
      "\n",
      "\n",
      "Layer number 4: torch.Size([8, 13])\n",
      "L1 norm: 13.951150894165039\n",
      "\n",
      "\n",
      "Layer number 5: torch.Size([4, 8])\n",
      "L1 norm: 4.625356197357178\n",
      "\n",
      "\n",
      "Layer number 1: torch.Size([11, 5])\n",
      "Frobenius norm: 1.9393653869628906\n",
      "\n",
      "\n",
      "Layer number 2: torch.Size([16, 11])\n",
      "Frobenius norm: 2.3132240772247314\n",
      "\n",
      "\n",
      "Layer number 3: torch.Size([13, 16])\n",
      "Frobenius norm: 2.047039747238159\n",
      "\n",
      "\n",
      "Layer number 4: torch.Size([8, 13])\n",
      "Frobenius norm: 1.5619230270385742\n",
      "\n",
      "\n",
      "Layer number 5: torch.Size([4, 8])\n",
      "Frobenius norm: 0.9907125234603882\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections.abc import Iterable\n",
    "def pretty_print(obj, title=None, norm = 'L1'):\n",
    "    if title is not None:\n",
    "        if isinstance(title, Iterable):\n",
    "            for count, t in enumerate(title):\n",
    "                print(f'Layer number {count + 1}: {t}')\n",
    "                print(f'{norm} norm: {(obj[count])}')\n",
    "                print('\\n')\n",
    "        else:    \n",
    "            print(title)\n",
    "            print(obj)\n",
    "    return None   \n",
    "\n",
    "# Now, let's calculate the L1 norm of the weihgts for each layer\n",
    "# print([val for val in my_model.state_dict().values()])  # Complete list of weights for each layer\n",
    "W_L1_norm = [torch.norm(val, 1) for val in my_model.state_dict().values()] # Full L1 norm for each layer\n",
    "W_L2_norm = [torch.norm(val, 'fro') for val in my_model.state_dict().values()]\n",
    "model_size = [var.size() for var in my_model.state_dict().values()]\n",
    "pretty_print(W_L1_norm, model_size)\n",
    "pretty_print(W_L2_norm, model_size, norm = 'Frobenius')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2502, 0.2565, 0.2445, 0.2488],\n",
      "        [0.2507, 0.2519, 0.2478, 0.2496],\n",
      "        [0.2500, 0.2501, 0.2500, 0.2499],\n",
      "        [0.2503, 0.2515, 0.2485, 0.2497],\n",
      "        [0.2490, 0.2566, 0.2474, 0.2469],\n",
      "        [0.2499, 0.2515, 0.2493, 0.2492],\n",
      "        [0.2499, 0.2538, 0.2470, 0.2493],\n",
      "        [0.2505, 0.2518, 0.2480, 0.2497],\n",
      "        [0.2503, 0.2521, 0.2481, 0.2494],\n",
      "        [0.2492, 0.2521, 0.2502, 0.2485]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "\n",
      "The most supported result for the input 0 is:  Cat, with probability:  0.256\n",
      "The most supported result for the input 1 is:  Cat, with probability:  0.252\n",
      "The most supported result for the input 2 is:  Cat, with probability:  0.25\n",
      "The most supported result for the input 3 is:  Cat, with probability:  0.251\n",
      "The most supported result for the input 4 is:  Cat, with probability:  0.257\n",
      "The most supported result for the input 5 is:  Cat, with probability:  0.252\n",
      "The most supported result for the input 6 is:  Cat, with probability:  0.254\n",
      "The most supported result for the input 7 is:  Cat, with probability:  0.252\n",
      "The most supported result for the input 8 is:  Cat, with probability:  0.252\n",
      "The most supported result for the input 9 is:  Cat, with probability:  0.252\n"
     ]
    }
   ],
   "source": [
    "# Obtain the output of 10 random datapoints\n",
    "out_model = my_model(torch.randn((10, 5)))\n",
    "\n",
    "print(out_model)      # print the obtained output\n",
    "print('\\n')\n",
    "\n",
    "# Obtain the most supported result by our model\n",
    "val, idx = torch.max(out_model, 1)\n",
    "\n",
    "my_dict = {0: 'Dog', 1: 'Cat', 2: 'Horse', 3: 'Bird'}\n",
    "for i in range(len(val)):\n",
    "    print(f'The most supported result for the input {i} is:  {my_dict[idx[i].item()]}, with probability:  {round(val[i].item(), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Log Loss: 1.3840515613555908\n",
      "Standard accuracy: 0.3\n"
     ]
    }
   ],
   "source": [
    "# Here we compute the goodness of our model\n",
    "def Log_Loss_acc(out_model, target):\n",
    "    nll_loss = torch.nn.NLLLoss()\n",
    "    return nll_loss(out_model, target)\n",
    "\n",
    "def accuracy(out_model, target):\n",
    "    return (torch.max(out_model, 1)[1] == target).sum().item()/len(target)\n",
    "    \n",
    "    \n",
    "out_model_log = torch.log(out_model) \n",
    "out_true = torch.randint(0, 4, (10,))\n",
    "print(f'Accuracy Log Loss: {Log_Loss_acc(out_model_log, out_true)}')\n",
    "print(f'Standard accuracy: {accuracy(out_model, out_true)}')\n",
    "\n",
    "# For more completeness we could also add functions to compute the ROC curve, the AUC score and so on"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd38a29cff24502477691f52d29271db08b26de3a0d5612e4dc14a190d76940"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('DL_units')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
